{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_day4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTgDWz6RP64H6lWdPjYgrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/deep_learning_day3_day4/blob/main/deep_learning_day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWPLGoBA1qyv"
      },
      "source": [
        "# 深層学習day4 レポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakCaaLplhgU"
      },
      "source": [
        "## 1. 強化学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sV4gOaDtklr"
      },
      "source": [
        "### 用語  \n",
        "AWS DeepRacerを例に説明する。  \n",
        "・エージェント（Agent）： 車（Vehicle）＝自分  \n",
        "・環境（Environment）： コース（Track）  \n",
        "・状態（State）： コースの中で自分がどこにいるか。通常、変数名はS。  \n",
        "・行動（Action）： 自分がどう進むか（どう走行するか）。  \n",
        "・報酬（Reward）： 行動の指標になるもの。通常、変数名はR。  \n",
        "・t： 現在の行動ステップ番号。「t」は時刻（time）を表す。  \n",
        "・t+1： 次の行動ステップ番号。Rt+1で、「次の行動に対する報酬」を意味する。  \n",
        "・報酬関数（Reward function）： 報酬値を定義する関数。  \n",
        "・価値関数（Value function）： 最大の報酬が得られる価値（Value）を表現する関数。  \n",
        "・方策関数（Policy function）： ある状態から最適な行動が得られる方策（Policy）を表現する関数。  \n",
        "・エピソード（Episode）： エージェントのスタートに始まり、完走、もしくはリタイアするまでの全ステップの経験のこと。  \n",
        "・ステップ（Step）：1つのステップは、状態＋行動＋報酬の1セットからなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvk1e-0A4tyN"
      },
      "source": [
        "### policy gradientアルゴリズム  \n",
        "ゴール：累積報酬の最大化。  \n",
        "\n",
        "現在のpolicyを利用してエピソードを収集。  \n",
        "↓  \n",
        "推定の累積報酬の勾配を計算。  \n",
        "↓  \n",
        "勾配の方向へと方策を更新。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYiTycgQlkl9"
      },
      "source": [
        "## 2. Alpha Go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2D-cCctlxr"
      },
      "source": [
        "Lee  \n",
        "ver.2。世界最強棋士Leeに勝ったことから。  \n",
        "### 呼吸点  \n",
        "石はタテ線ヨコ線の方向が空いていれば呼吸できる。 石のタテヨコ（上下左右）の空きを呼吸点という。  \n",
        "アタリ  \n",
        "あと一か所で窒息  \n",
        "シチョウ  \n",
        "アタリの連続で、逃げる側は盤がある限りは逃げることができるのだが、盤の端まで到達してしまうとどうにもできず、石を取られてしまう状態  \n",
        "連  \n",
        "縦横につながった同じ色の石の集合。コンピュータ囲碁だけの言葉。  \n",
        "手番  \n",
        "打つ番。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckBAR7VwWLjv"
      },
      "source": [
        "ゲーム木，根ノード，葉ノード  \n",
        "RollOutPolicy\n",
        "決着がつくまでランダムに手を進め、その勝敗をそのまま報酬に \n",
        "\n",
        "### モンテカルロ木探索  \n",
        "モンテカルロ法  \n",
        "完全ランダム選択 → 最善の手を選択できない  \n",
        "モンテカルロ法にUCB（Upper Confidence Bound）アルゴリズムを取り入れた  \n",
        "\n",
        "https://blog.brainpad.co.jp/entry/2018/04/05/163000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEyYNgzvLDAi"
      },
      "source": [
        "AlphaGoZeroおよびAlphaZero  \n",
        "Self-Play  \n",
        "モンテカルロ木探索により手を決定し、自分自身と何度も戦う。  \n",
        "Neural Network Training  \n",
        "ネットワークの入力：盤面の状態s  \n",
        "出力は２つ  \n",
        "### policy network  \n",
        "その状態からどの手を選択すべきかという、各手の着手確率  \n",
        "### value network  \n",
        "その状態の価値、つまりその状態で当該プレイヤーが勝つ確率  \n",
        "\n",
        "ResidualNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VTDc0Uak6BR"
      },
      "source": [
        "## 3. 軽量化・高速化技術"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drlaEYZRoh83"
      },
      "source": [
        "### データ並列化  \n",
        "モデルを複数の計算機にコピーし、分割したデータを各計算機に計算させる。  \n",
        "同期型・非同期型  \n",
        "各計算機のパラメータの合わせ方  \n",
        "データが大きい時はデータ並列化が良い。\n",
        "### モデル並列化  \n",
        "モデルを複数の計算機に分割  \n",
        "モデルが大きい時はモデル並列化が良い。\n",
        "### GPU\n",
        "CPU  \n",
        "•高性能なコアが少数  \n",
        "•複雑で連続的な処理が得意  \n",
        "GPU  \n",
        "•比較的低性能なコアが多数  \n",
        "•簡単な並列処理が得意  \n",
        "→ NNは単純な行列演算が多いので高速化が可能！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEKckGuVx-H5"
      },
      "source": [
        "### 量子化（Quantization）  \n",
        "重みの精度を下げる → 少ないメモリで高速計算  \n",
        "パラメータの64 bit 浮動小数点を32 bit にする等\n",
        "\n",
        "### 蒸留（Distillation）  \n",
        "key word 『知識の継承』  \n",
        "教師モデル：複雑で高精度  \n",
        "↓  \n",
        "生徒モデル：教師モデルをもとに作られる軽量なモデル  \n",
        "### プルーニング（Pruning）  \n",
        "寄与の少ないニューロンをモデルから削減  \n",
        "→ 少ないメモリで高速計算  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KocnhTbrlE3g"
      },
      "source": [
        "## 4. 応用モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-TopgsmOF7"
      },
      "source": [
        "### MobileNets  \n",
        "https://www.slideshare.net/harmonylab/mobilenet-81645825  \n",
        "モバイル端末で動くCNN  \n",
        "サイズ小さく、処理速度早く  \n",
        "  \n",
        "Depthwise separatable convolution  \n",
        "畳み込みを分けると計算量が減る。  \n",
        "Depthwise Convolution … 空間方向の畳み込み  \n",
        "Pointwise Convolution … チャネル方向の畳み込み  \n",
        "\n",
        "ハイパーパラメータ  \n",
        "Width Multiplier  \n",
        "Resolution Multiplier  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koWGShUbWSRu"
      },
      "source": [
        "### DenseNetとResNet  \n",
        "https://deepsquare.jp/2020/04/resnet-densenet/  \n",
        "ともにより深い層のCNNの実現を目指したもの  \n",
        "\n",
        "### Residual Network （ResNet）  \n",
        "shortcut connection … 手前の層の入力を後ろの層に直接足し合わせることで勾配消失問題を解決。  \n",
        "名前の由来 … 残差(residue)を出力  \n",
        "\n",
        "### DenseNet  \n",
        "ResNetの改良版。  \n",
        "レイヤー間の情報の伝達を最大化するためにすべての特徴量サイズが同じレイヤーを結合  \n",
        "名前の由来 … レイヤー間が密に結合していることから  \n",
        "Dense Block  \n",
        "Transition layer  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbmWOJFgcOuR"
      },
      "source": [
        "### Batch Normalizationの問題点  \n",
        "ミニバッチごとに平均・分散を計算するため、ミニ・バッチが小さい場合、平均・分散が不安定になる。  \n",
        "RNNでは各サンプルごとに文章の長さが違い、学習データよりもテストデータが長いこともあり、適用が簡単ではない。  \n",
        "\n",
        "### LayerNorm  \n",
        "1つのサンプルにおける各レイヤーの隠れ層の値の平均・分散で正規化する。  \n",
        "RNNにはLayerNormを使うのが良い。\n",
        "\n",
        "https://data-analytics.fun/2020/07/16/understanding-layer-normalization/  \n",
        "\n",
        "### InstanceNorm  \n",
        "Batch Normalizationのバッチサイズが1の場合と同じ。  \n",
        "スタイル変換タスク（GAN）に適しているそうだ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSmDu5Bjk5f8"
      },
      "source": [
        "## 5. Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp5p0w2MF9EX"
      },
      "source": [
        "Transformerとは  \n",
        "計算速度向上のため、RNNを使わずにAttentionを使って並列計算するモデル\n",
        "\n",
        "\"Attention Is All You Need\" 論文名？なんか調子いいね笑  \n",
        "\n",
        "PositionalEncoding層の採用  \n",
        "「文全体における単語の位置情報」  \n",
        "RNNを使わないことで失われる → 入力する単語データに埋め込む。  \n",
        "\n",
        "AttentionにおけるQuery-Key-Valueモデルの採用  \n",
        "初期のAttentionは単純なSource-Target型  \n",
        "単語同士の照応関係（アライメント）をより正確に反映することができるように。\n",
        "\n",
        "TransformerはSeq2seqと同じでEncoderとDecoderを持つ。\n",
        "\n",
        "Self-Attention層 … 入力文章内の照応関係（類似度や重要度）を獲得  \n",
        "\n",
        "MultiHead-Attention層 … 並列処理を実施。またアンサンブル学習の様になり翻訳の質の向上が期待できる。\n",
        "\n",
        "Masked Multi-Head Attention層 … PositionalEncoding層によって受け取ってしまう情報を隠す（カンニングの様な状態になることを避けるため）\n",
        "\n",
        "https://deepsquare.jp/2020/07/transformer/#outline__4_2_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFlNpN5ulUl4"
      },
      "source": [
        "## 6.物体検知・セグメンテーション"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjGWg1XqznvH"
      },
      "source": [
        "TP(True Positive)：検査結果はコ〇ナで、実際もコ〇ナだった。  \n",
        "FP(False Positive)：検査結果はコ〇ナだったが、実際はコ〇ナではなかった。  \n",
        "FN(False Negative)：検査結果はコ〇ナではなかったが、実際はコ〇ナだった。  \n",
        "TN(True Negative)：検査結果はコ〇ナではなく、実際もコ〇ナではなかった。\n",
        "\n",
        "Precision：全ての検査結果（陽性，陰性両方の結果）の内、正しくコ〇ナと検査できた割合。  \n",
        "Precisionは「予測結果の精度」を見るための指標。大きな値であるほど予測した結果が確からしい。  \n",
        "\n",
        "Recall：実際はコ〇ナであった全ての結果の内、陽性と正しく検査できた割合。  \n",
        "Recallは「取りこぼしのなさ」を見るための指標。大きな値であるほど取りこぼしが少ない。\n",
        "\n",
        "Precisionだけだと「実際にはコ〇ナであったのに検査結果がコ〇ナでないとして取りこぼした場合」の精度評価を測れない → そこでRecall\n",
        "\n",
        "IoU：Intersection over Union  \n",
        "\n",
        "BB … bounding box  \n",
        "Ground-Truth BB … 正解の四角枠  \n",
        "Predicted BB … 予測した四角枠  \n",
        "\n",
        "IoUは、「領域の積：Intersection」を「領域の和：Union」で「割る：over」した値。  \n",
        "\n",
        "２つの領域がどれだけ重なっているのか、0～1の値で見れる。\n",
        "\n",
        "https://qiita.com/cv_carnavi/items/08e11426e2fac8433fed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO2Y361yI3cz"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pAk0soJI6P4"
      },
      "source": [
        "### ミニマックス探索(minimax game)  \n",
        "$$\\min_{G} \\max_{D} V(D, G)$$\n",
        "$V(D, G)$を最大化させる$D$があって、さらにこの$V(D, G)$を最小化する$G$がある。  \n",
        "「想定される最大の損害が最小になるように決断を行う戦略」のこと。  \n",
        "\n",
        "$D$：discriminator 識別器  \n",
        "$G$：generator 生成器\n",
        "\n",
        "### ミニマックス探索がなぜ「GANの数式」となるのか？  \n",
        "識別器が$V(D, G)$を最大化し、生成器が$V(D, G)$を最小化すること  \n",
        "↓↑  \n",
        "GANが驚異的なまでに精巧な合成写真を生成できること  \n",
        "\n",
        "これらの間にはいったいどのような関係が？  \n",
        "\n",
        "### ゼロサムゲーム  \n",
        "ゼロサムゲームとは、二人でプレーするゲームで合って、相手のスコアと自分のスコアの和が常に $0$ になるようなゲームのこと。  \n",
        "自分のスコアが $x$ 点だった時、相手のスコアは確実に $-x$ 点。  \n",
        "「自分が $x$ だけ得すること」と「相手が $-x$ だけ損すること」は本質的に同じ。  \n",
        "この考え方では、  \n",
        "Gの立場から見ると$V(D, G)$とは「損」のことであり、  \n",
        "逆にDの立場から見ると$V(D, G)$とは「得」のことである。  \n",
        "\n",
        "Dは識別性能を最大化する選択をし（その時Gは固定）、  \n",
        "それを受けてGは識別性能を最小化する選択をする。\n",
        "\n",
        "解釈  \n",
        "識別器の性能が高すぎる  \n",
        "→ 生成器は学習機会を失う(「こんな感じだと騙せる」という経験が積めない)。  \n",
        "識別器の性能が低すぎる  \n",
        "→ 生成器は低レベルなフェイクで満足してしまう。  \n",
        "\n",
        "「バランスの取れた」性能になってもらう為に「フェアな勝負」で両者本気で戦って欲しい。この様な要請から、ミニマックス探索というアイデアが選ばれたのでは？\n",
        "\n",
        "https://912.hateblo.jp/entry/2020/06/19/141931"
      ]
    }
  ]
}