{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_hands_on_01.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/deep_learning_day3_day4/blob/main/pytorch_hands_on_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4JztMOxNIvu"
      },
      "source": [
        "# 連載『PyTorch入門』のノートブック（1）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzlYFotSTzy"
      },
      "source": [
        "※上から順に実行してください。上のコードで実行したものを再利用しているところがあるため、すべて実行しないとエラーになるコードがあります。  \n",
        "　すべてのコードを一括実行したい場合は、メニューバーから［ランタイム］－［すべてのセルを実行］をクリックしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8psaSQlSFN9"
      },
      "source": [
        "※「Python 3」を利用してください。  \n",
        "　Python 3を利用するには、メニューバーから［ランタイム］－［ランタイムのタイプを変更］を選択すると表示される［ノートブックの設定］ダイアログの、［ランタイムのタイプ］欄で「Python 3」に選択し、その右下にある［保存］ボタンをクリックしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ueKLtX95oVW"
      },
      "source": [
        "# 第1回　難しくない！　PyTorchでニューラルネットワークの基本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_E3IdevRyna"
      },
      "source": [
        "## ■PyTorchとは？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQwqGYlhR7-J"
      },
      "source": [
        "- 人気急上昇中（参考：「[PyTorch vs. TensorFlow、ディープラーニングフレームワークはどっちを使うべきか問題 (1/2)：気になるニュース＆ネット記事 - ＠IT](https://www.atmarkit.co.jp/ait/articles/1910/31/news028.html)）」\n",
        "- Pythonic（＝Pythonのイディオムをうまく活用した自然なコーディングが可能）\n",
        "- 柔軟性や拡張性に優れる（特に“define-by-run”：実行しながら定義／eager execution：即時実行なので、例えばモデルのフォワードプロパゲーション（順伝播）時にif条件やforループなどの制御フローを書いて動的に計算グラフを変更したりできる）\n",
        "\n",
        "特にNLP（Natural Language Processing：自然言語処理）の分野では、研究者はさまざまな長さの文を訓練する必要性があるため、動的な計算グラフが不可欠。実際に「PyTorchがデファクトスタンダードになっている」と筆者が初めて聞いたのは、NLPに関しての話だった。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdujIPyYdDT"
      },
      "source": [
        "## ■本稿の目的と方針"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Grrfk74Yl7a"
      },
      "source": [
        "PyTorchでニューラルネットワークを定義するための最重要の基礎知識を最短で紹介する。\n",
        "\n",
        "- PyTorchのチュートリアルは最初からCNNで最初から複雑（※これはある程度のニューラルネットワークの知識がない人を門前払いする意味があると思う）\n",
        "- まずはニューラルネットワークの原型「ニューロン」を実装することで、核となる機能を理解する\n",
        "- 「ニューロン」「活性化関数」「正則化」「勾配」「確率的勾配降下法（SGD）」といった概念が分からない場合は、『[TensorFlow 2＋Keras（tf.keras）入門 - ＠IT](https://www.atmarkit.co.jp/ait/subtop/features/di/tf2keras_index.html)』の第1回～第3回で挙動を示しながら分かりやすく説明しているので、先にそちらを一読してほしい\n",
        "- 最終的には、基本的なニューラルネットワーク＆ディープラーニングのコードが思いどおりに書けるようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMyXEPUNmKU"
      },
      "source": [
        "## ■本稿で説明する大まかな流れ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOYuPJ6Ngnu"
      },
      "source": [
        "- （1）ニューロンのモデル定義\n",
        "- （2）フォワードプロパゲーション（順伝播）\n",
        "- （3）バックプロパゲーション（逆伝播）と自動微分（Autograd）\n",
        "- （4）PyTorchの基礎： テンソルとデータ型\n",
        "- （5）データセットとデーターローダー（DataLoader）\n",
        "- （6）ディープニューラルネットのモデル定義\n",
        "- （7）学習／最適化（オプティマイザー）\n",
        "- （8）評価／精度検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC_Xm6Ouqk9"
      },
      "source": [
        "## ■（1）ニューロンのモデル定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMboLmdvjYaq"
      },
      "source": [
        "###  【チェック】Pythonバージョン（※3系を使うこと）\n",
        "Colabにインストール済みのものを使う。もし2系になっている場合は、メニューバーの［ランタイム］－［ランタイムのタイプを変更］をクリックして切り替えてほしい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziRZWCujWXN"
      },
      "source": [
        "import sys\n",
        "print('Python', sys.version)\n",
        "# Python 3.6.9 (default, Nov  7 2019, 10:44:02)  …… などと表示される"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDCm1WGuu2OW"
      },
      "source": [
        "###  【チェック】PyTorchバージョン\n",
        "基本的にはColabにインストール済みのものを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EuZVjZZu663"
      },
      "source": [
        "import torch\n",
        "print('PyTorch', torch.__version__)\n",
        "# PyTorch 1.3.1 ……などと表示される"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYTu1liZvPUq"
      },
      "source": [
        "### リスト1-0　［オプション］ライブラリ「PyTorch」最新バージョンのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ruhTNMvUQy"
      },
      "source": [
        "#!pip install torch        # ライブラリ「PyTorch」をインストール\n",
        "#!pip install torchvision  # 画像／ビデオ処理のPyTorch用追加パッケージもインストール\n",
        "\n",
        "# 最新バージョンにアップグレードする場合\n",
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "# バージョンを明示してアップグレードする場合\n",
        "#!pip install --upgrade torch===1.4.0 torchvision===0.5.0\n",
        "\n",
        "# 最新バージョンをインストールする場合\n",
        "#!pip install torch torchvision\n",
        "\n",
        "# バージョンを明示してインストールする場合\n",
        "#!pip install torch===1.4.0 torchvision===0.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDCmbMXF490y"
      },
      "source": [
        "このコードのポイント：\n",
        "- 「torchvision」パッケージは本稿では使っていないが、同時にインストールしないとパッケージ関係が不整合となるため、インストールしておく必要がある\n",
        "- 実行後にランタイムを再起動する必要がある"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzWIxm9wwfpZ"
      },
      "source": [
        "### ［オプション］【チェック】PyTorchバージョン（※インストール後の確認）\n",
        "バージョン1.4.0以上になっているか再度チェックする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTgmXjFewkVv"
      },
      "source": [
        "import torch\n",
        "print('PyTorch', torch.__version__)\n",
        "# PyTorch 1.4.0 ……などと表示される"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3nDS-MFxo06"
      },
      "source": [
        "### リスト1-1　ニューロンのモデル設計と活性化関数\n",
        "\n",
        "- ニューロンへの入力＝$(w_1 \\times X_1)+(w_2 \\times X_2)+b$\n",
        "- ニューロンからの出力＝$a((w_1 \\times X_1)+(w_2 \\times X_2)+b)$\n",
        "  - $a()$は活性化関数を意味する。つまりニューロンの入力結果を、活性化関数で変換したうえで、出力する\n",
        "  - 今回の活性化関数は、**tanh**関数とする\n",
        "- ニューロンの構造とデータ入力：座標$(X_1, X_2)$\n",
        "  - 入力の数（`INPUT_FEATURES`）は、$X_1$と$X_2$で**2つ**\n",
        "  - ニューロンの数（`OUTPUT_NEURONS`）は、**1つ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uu-jPQOxy2K"
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 定数（モデル定義時に必要となるもの）\n",
        "INPUT_FEATURES = 2  # 入力（特徴）の数： 2\n",
        "OUTPUT_NEURONS = 1  # ニューロンの数： 1\n",
        "\n",
        "# 変数（モデル定義時に必要となるもの）\n",
        "activation = torch.nn.Tanh()  # 活性化関数： tanh関数\n",
        "\n",
        "# 「torch.nn.Moduleクラスのサブクラス化」によるモデルの定義\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # 層（layer：レイヤー）を定義\n",
        "        self.layer1 = nn.Linear(  # Linearは「全結合層」を指す\n",
        "            INPUT_FEATURES,       # データ（特徴）の入力ユニット数\n",
        "            OUTPUT_NEURONS)       # 出力結果への出力ユニット数\n",
        "\n",
        "    def forward(self, input):\n",
        "        # フォワードパスを定義\n",
        "        output = activation(self.layer1(input))  # 活性化関数は変数として定義\n",
        "        # 「出力＝活性化関数（第n層（入力））」の形式で記述する。\n",
        "        # 層（layer）を重ねる場合は、同様の記述を続ければよい（第3回＝後述）。\n",
        "        # 「出力（output）」は次の層（layer）への「入力（input）」に使う。\n",
        "        # 慣例では入力も出力も「x」と同じ変数名で記述する（よって以下では「x」と書く）\n",
        "        return output\n",
        "\n",
        "# モデル（NeuralNetworkクラス）のインスタンス化\n",
        "model = NeuralNetwork()\n",
        "model   # モデルの内容を出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9COzxIQFzqJQ"
      },
      "source": [
        "このコードのポイント：\n",
        "- `torch.nn.Module`クラスを継承して独自にモデル用クラスを定義する。Pythonの「モジュール」と紛らわしいので、本稿では「`torch.nn.Module`」と表記する\n",
        "    - `__init__`関数にレイヤー（層）を定義する\n",
        "    - `forward`関数にフォワードパス（＝活性化関数で変換しながらデータを流す処理）を実装する\n",
        "    - ちなみにバックプロパゲーション（誤差逆伝播）のための`backward`関数は自動微分機能により自動作成される（後述）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmXJJjRA5RI"
      },
      "source": [
        "#@title tanh関数\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "x = np.arange(-6.0, 6.0, 0.001)\n",
        "plt.plot(x, sigmoid(x), label = \"Sigmoid\")\n",
        "plt.plot(x, tanh(x), label = \"tanh\")\n",
        "plt.xlim(-6, 6)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxdAFpl2TYr7"
      },
      "source": [
        "- PyTorchでは、以下の活性化関数が用意されている\n",
        "  - ELU\n",
        "  - Hardshrink\n",
        "  - Hardtanh\n",
        "  - LeakyReLU\n",
        "  - LogSigmoid\n",
        "  - MultiheadAttention\n",
        "  - PReLU\n",
        "  - ReLU（有名）\n",
        "  - ReLU6\n",
        "  - RReLU\n",
        "  - SELU\n",
        "  - CELU\n",
        "  - GELU\n",
        "  - Sigmoid（シグモイド）\n",
        "  - Softplus（ソフトプラス）\n",
        "  - Softshrink\n",
        "  - Softsign（ソフトサイン）\n",
        "  - Tanh（本稿で使用）\n",
        "  - Tanhshrink\n",
        "  - Threshold\n",
        "  - Softmin\n",
        "  - Softmax\n",
        "  - Softmax2d\n",
        "  - LogSoftmax\n",
        "  - AdaptiveLogSoftmaxWithLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bp5s_s611Lq"
      },
      "source": [
        "### リスト1-2　パラメーター（重みとバイアス）の初期値設定\n",
        "\n",
        "- $w_1=0.6$、$w_2=-0.2$、$b=0.8$と仮定して、ニューロンのモデルを定義\n",
        "  - ※これらの値は通常は学習により決定されるが、今回は未学習なので仮の固定数値としている\n",
        "  - 重さ（$w_1$と$w_2$）は2次元配列でまとめて表記する： `weight_array`\n",
        "    - 通常は、ニューロンは複数あるので、2次元配列で表記する\n",
        "    - 複数の重みが「行」を構成し、複数のニューロンが「列」を構成する\n",
        "    - 今回は、重みが**2つ**で、ニューロンが**1つ**なので、**2行1列**で記述する\n",
        "    -  `[[ 0.6],`<br>&nbsp;&nbsp;`[-0.2]]`\n",
        "  - バイアス（$b$）は1次元配列でまとめて表記する： `bias_array`\n",
        "    - `[0.8]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3iD9H91qON"
      },
      "source": [
        "# パラメーター（ニューロンへの入力で必要となるもの）の定義\n",
        "weight_array = nn.Parameter(\n",
        "    torch.tensor([[ 0.6,\n",
        "                   -0.2]]))  # 重み\n",
        "bias_array = nn.Parameter(\n",
        "    torch.tensor([  0.8 ]))  # バイアス\n",
        "\n",
        "# 重みとバイアスの初期値設定\n",
        "model.layer1.weight = weight_array\n",
        "model.layer1.bias = bias_array\n",
        "\n",
        "# torch.nn.Module全体の状態を辞書形式で取得\n",
        "params = model.state_dict()\n",
        "#params = list(model.parameters()) # このように取得することも可能\n",
        "params\n",
        "# 出力例：\n",
        "# OrderedDict([('layer1.weight', tensor([[ 0.6000, -0.2000]])),\n",
        "#              ('layer1.bias', tensor([0.8000]))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQOf7w_K30L9"
      },
      "source": [
        "このコードのポイント：\n",
        "- モデルのパラメーターは`torch.nn.Parameter`オブジェクトとして定義する\n",
        "  - `torch.nn.Parameter`クラスのコンストラクター（厳密には`__init__`関数）には`torch.Tensor`オブジェクト（以下、テンソル）を指定する\n",
        "  - `torch.Tensor`のコンストラクターにはPythonの多次元リストを指定できる\n",
        "  - NumPyの多次元配列からのテンソルの作成や、テンソルの使い方については第2回（＝後述）\n",
        "- 重みやバイアスの初期値設定：\n",
        "  - `＜モデル名＞.＜レイヤー名＞.weight`プロパティに重みが指定できる\n",
        "  - `＜モデル名＞.＜レイヤー名＞.baias`プロパティにバイアスが指定できる\n",
        "  - 通常は「**0**」や「一様分布の**ランダムな値**」などを指定する（第3回＝後述）\n",
        "- 重みやバイアスといったパラメーターなどの`torch.nn.Module`全体の状態は、`＜モデル名＞.state_dict()`メソッドで取得できる\n",
        "  - ちなみにパラメーターを最適化で使う際は、`＜モデル名＞.parameters()`メソッドで取得する（第3回＝後述）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuu22vpliye3"
      },
      "source": [
        "## ■（2）フォワードプロパゲーション（順伝播）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd1IafG72_SS"
      },
      "source": [
        "### リスト2-1　フォワードプロパゲーションの実行と結果確認\n",
        "- ニューロンに、座標$(X_1, X_2)$データを入力する\n",
        "  - 通常のデータは表形式（＝2次元配列）だが、今回は$(1.0, 2.0)$という1つのデータ\n",
        "    - 1つのデータでも2次元配列（具体的には**1行2列**）で表現する必要がある"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0vt-HN3HVx"
      },
      "source": [
        "X_data = torch.tensor([[1.0, 2.0]])  # 入力する座標データ（1.0、2.0）\n",
        "print(X_data)\n",
        "# tensor([[1., 2.]]) ……などと表示される\n",
        "\n",
        "y_pred = model(X_data)  # このモデルに、データを入力して、出力を得る（＝予測：predict）\n",
        "print(y_pred)\n",
        "# tensor([[0.7616]], grad_fn=<TanhBackward>) ……などと表示される"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eg6v2OEmg8x"
      },
      "source": [
        "このコードのポイント：\n",
        "- フォワードプロパゲーション（順伝播）で、データ（`X_data`）を入力し、モデル（`model`）が推論した結果（`y_pred`）を出力している\n",
        "- その結果の数値は、手動で計算した値（`0.7616`）と同じになるのが確認できるはず\n",
        "- `grad_fn`属性（この例では「TanhBackward」）には、勾配（偏微分）などを計算するための関数が自動作成されている。バックプロパゲーション（逆伝播）による学習の際に利用される"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHxRfDZfjOiq"
      },
      "source": [
        "### リスト2-2　動的な計算グラフの可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Mo-cBPlFqB"
      },
      "source": [
        "「[torchviz · PyPI](https://pypi.org/project/torchviz/)」をインストールして、動的な計算グラフ（dynamic computation graph）を可視化する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luqbk_p352Qs"
      },
      "source": [
        "!pip install torchviz          # 初回の「torchviz」パッケージインストール時にのみ必要"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPPHf3GGhUBS"
      },
      "source": [
        "from torchviz import make_dot  # 「torchviz」モジュールから「make_dot」関数をインポート\n",
        "make_dot(y_pred, params=dict(model.named_parameters()))\n",
        "# 引数「params」には、全パラメーターの「名前: テンソル」の辞書を指定する。\n",
        "# 「dict(model.named_parameters())」はその辞書を取得している"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9TQTF59l8AK"
      },
      "source": [
        "この図のポイント：\n",
        "- 青色のボックス： 勾配を計算する必要がある、重みやバイアスなどのパラメーター。この例では`(1, 2)`が重みで、`(1)`がバイアス\n",
        "- 灰色のボックス： 勾配（偏微分）などを計算するための関数。「テンソル」データの`grad_fn`属性（この例では「TBackward」や「AddmmBackward」）に自動作成されている。バックプロパゲーション（逆伝播）による学習の際に利用される\n",
        "- 緑色のボックス： グラフ計算の開始点。`backward()`メソッドを呼び出すと、ここから逆順に計算していく。内容は灰色のボックスと同じ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFwzuitssETB"
      },
      "source": [
        "## ■（3）バックプロパゲーション（逆伝播）と自動微分（Autograd）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_trGfyZBB0jX"
      },
      "source": [
        "### リスト3-1　簡単な式で自動微分してみる\n",
        "\n",
        "`backward()`メソッドでバックプロパゲーション（誤差逆伝播）をさせる。ニューラルネットワークの誤差逆伝播では、「微分係数（derivative）の計算」という面倒くさい処理が待っている。ディープラーニングのライブラリは、この処理を自動化してくれるので大変便利である。この機能を「自動微分（AD： Automatic differentiation）」や「Autograd」（gradients computed automatically： 自動計算された勾配）などと呼ぶ。\n",
        "\n",
        "ちなみに詳細を知る必要はあまりないが、`torch.autograd`モジュールは厳密には「リバースモードの自動微分」機能を提供しており、vector-Jacobian product（VJP：ベクトル-ヤコビアン積）と呼ばれる計算を行うエンジンである（参考「[Autograd: Automatic Differentiation — PyTorch Tutorials 1.4.0 documentation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)」、論文「[Automatic differentiation in PyTorch | OpenReview](https://openreview.net/forum?id=BJJsrmfCZ)」）。\n",
        "\n",
        "PyTorchの自動微分（Autograd）機能を、非常にシンプルな例で示しておく。\n",
        "\n",
        "- 計算式： $y=x^2$\n",
        "- 導関数： $\\frac{dy}{dx}=2x$ （ $y$ を $x$ で微分する）\n",
        "- 例えば $x$ が__1.0__の地点の勾配（＝接線の傾き）は__2.0__となる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-lnNEoOEYfz"
      },
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)  # 今回は入力に勾配（gradient）を必要とする\n",
        "# 「requires_grad」が「True」（デフォルト：False）の場合、\n",
        "# torch.autogradが入力テンソルに関するパラメーター操作（勾配）を記録するようになる\n",
        "\n",
        "#x.requires_grad_(True)  # 「requires_grad_()」メソッドで後から変更することも可能\n",
        "\n",
        "y = x ** 2     # 「yイコールxの二乗」という計算式の計算グラフを構築\n",
        "print(y)       # tensor(1., grad_fn=<PowBackward0>) ……などと表示される\n",
        "\n",
        "y.backward()   # 逆伝播の処理として、上記式から微分係数（＝勾配）を計算（自動微分：Autograd）\n",
        "\n",
        "g = x.grad     # 与えられた入力（x）によって計算された勾配の値（grad）を取得\n",
        "print(g)       # tensor(2.)  ……などと表示される\n",
        "# 計算式の微分係数（＝勾配）を計算するための導関数は「dy/dx=2x」なので、\n",
        "#「x=1.0」地点の勾配（＝接線の傾き）は「2.0」となり、出力結果は正しい。\n",
        "# 例えば「x=0.0」地点の勾配は「0.0」、「x=10.0」地点の勾配は「20.0」である"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKBUuoYOTnH4"
      },
      "source": [
        "このコードのポイント：\n",
        "- PyTorchが「自動微分」機能を持つライブラリであることが確認できた\n",
        "- 出力されたテンソル（`y`）の`backward()`メソッドでバックプロパゲーション（逆伝播）を実行できる。なお、ニューラルネットワークの場合は、損失を表すテンソルの`backward()`メソッドを呼び出すことになる\n",
        "  - 出力されたテンソルの計算式（`y`）を入力したテンソル(`x`)で微分計算している\n",
        "- 計算された微分係数（＝勾配：gradient）は、入力したテンソルの`grad`プロパティで取得できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtesDEFJoHLZ"
      },
      "source": [
        "### リスト3-2　ニューラルネットワークにおける各パラメーターの勾配"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbQ1R6nKnwoC"
      },
      "source": [
        "# 勾配計算の前に、各パラメーター（重みやバイアス）の勾配の値（grad）をリセットしておく\n",
        "model.layer1.weight.grad = None      # 重み\n",
        "model.layer1.bias.grad = None        # バイアス\n",
        "#model.zero_grad()                   # これを呼び出しても上記と同じくリセットされる\n",
        "\n",
        "X_data = torch.tensor([[1.0, 2.0]])  # 入力データ（※再掲）\n",
        "y_pred = model(X_data)               # 出力結果（※再掲）\n",
        "y_true = torch.tensor([[1.0]])       # 正解ラベル\n",
        "\n",
        "criterion = nn.MSELoss()             # 誤差からの損失を測る「基準」＝損失関数\n",
        "loss = criterion(y_pred, y_true)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "loss.backward()   # 逆伝播の処理として、勾配を計算（自動微分：Autograd）\n",
        "\n",
        "# 勾配の値（grad）は、各パラメーター（重みやバイアス）から取得できる\n",
        "print(model.layer1.weight.grad) # tensor([[-0.2002, -0.4005]])  ……などと表示される\n",
        "print(model.layer1.bias.grad)   # tensor([-0.2002])  ……などと表示される\n",
        "# ※パラメーターは「list(model.parameters())」で取得することも可能"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvSX4RtIpx8U"
      },
      "source": [
        "このコードのポイント：\n",
        "- `criterion`に損失関数を代入するのが定石\n",
        "- `backward`メソッドによるバックプロパゲーション\n",
        "- この例では単純にするために1回しか処理してないが、本来はミニバッチのイテレーションや全体のエポックの回数繰り返し処理必要がある（第3回＝後述）\n",
        "- モデルにおける各パラメーター（`weight`や`bias`）の`grad`プロパティから、勾配の値は取得できる"
      ]
    }
  ]
}