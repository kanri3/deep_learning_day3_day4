{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_hands_on_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/deep_learning_day3_day4/blob/main/pytorch_hands_on_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W2oc9K0KxGz"
      },
      "source": [
        "# 第3回　PyTorchによるディープラーニング実装手順の基本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVfZeXrgYg7m"
      },
      "source": [
        "## ■（5）データセットとデーターローダー（DataLoader）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOb1W3i57oGe"
      },
      "source": [
        "「[第1回　初めてのニューラルネットワーク実装、まずは準備をしよう ― 仕組み理解×初実装（前編）：TensorFlow 2＋Keras（tf.keras）入門 - ＠IT](https://www.atmarkit.co.jp/ait/articles/1909/19/news026.html)」の記事と同じように、シンプルな座標点データを生成して使う。使い方は、前述の記事を参照してほしい。\n",
        "\n",
        "なお、座標点データは、「[ニューラルネットワーク Playground - Deep Insider](https://deepinsider.github.io/playground/)」（以下、Playground）と同じ生成仕様となっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2fU5kIiRher"
      },
      "source": [
        "### リスト5-1　座標点データの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnL886VJR-HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32346758-d5f7-4157-b51d-05e05140ae4d"
      },
      "source": [
        "# 座標点データを生成するライブラリのインストール\n",
        "!pip install playground-data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting playground-data\n",
            "  Downloading playground-data-1.1.1.tar.gz (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from playground-data) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from playground-data) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->playground-data) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->playground-data) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->playground-data) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->playground-data) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->playground-data) (1.15.0)\n",
            "Building wheels for collected packages: playground-data\n",
            "  Building wheel for playground-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playground-data: filename=playground_data-1.1.1-py2.py3-none-any.whl size=20798 sha256=f250dd21a70b6a4854dc36298f83a6dec1a0ef153a14126091203b0f187c6174\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/83/26/9701478cd2f31df42fcc7d2cf0fa3fd6ff23cf8e44346166f3\n",
            "Successfully built playground-data\n",
            "Installing collected packages: playground-data\n",
            "Successfully installed playground-data-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M_-s3ooRmVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651696d7-ad89-4c54-f1f4-7023f4ec358e"
      },
      "source": [
        "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
        "import plygdata as pg\n",
        "\n",
        "# 設定値を定数として定義\n",
        "PROBLEM_DATA_TYPE = pg.DatasetType.ClassifyCircleData # 問題種別：「分類（Classification）」、データ種別：「円（CircleData）」を選択\n",
        "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
        "DATA_NOISE = 0.0           # ノイズ： 0％\n",
        "\n",
        "# 定義済みの定数を引数に指定して、データを生成する\n",
        "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
        "\n",
        "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
        "X_train, y_train, X_valid, y_valid = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
        "\n",
        "# データ分割後の各変数の内容例として、それぞれ5件ずつ出力\n",
        "print('X_train:'); print(X_train[:5])\n",
        "print('y_train:'); print(y_train[:5])\n",
        "print('X_valid:'); print(X_valid[:5])\n",
        "print('y_valid:'); print(y_valid[:5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:\n",
            "[[-0.74603377 -0.83009882]\n",
            " [ 3.74373198 -0.43348149]\n",
            " [ 4.42617578  1.36991257]\n",
            " [-3.86253484 -0.64020839]\n",
            " [ 0.37063188  0.04299322]]\n",
            "y_train:\n",
            "[[ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]]\n",
            "X_valid:\n",
            "[[ 2.60863208e-01 -1.56930102e+00]\n",
            " [ 1.31089532e-02 -3.94723588e+00]\n",
            " [-3.24950024e-03 -2.54868653e-01]\n",
            " [-1.27034655e+00 -3.87277236e+00]\n",
            " [ 2.06355786e-01  4.69248472e-01]]\n",
            "y_valid:\n",
            "[[ 1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [ 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYfp-w3t-BjX"
      },
      "source": [
        "### リスト5-2　データセットとデーターローダーの作成\n",
        "\n",
        "PyTorchにはミニバッチを簡単に扱うための`DataLoader`クラスが用意されている。このクラスを利用するには、既存のデータや教師ラベルといったテンソルを1つの`TensorDataset`にまとめる必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Rj8ASo8ICO"
      },
      "source": [
        "# データ関連のユーティリティクラスをインポート\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "\n",
        "# 定数（学習方法設計時に必要となるもの）\n",
        "BATCH_SIZE = 15  # バッチサイズ： 15（Playgroundの選択肢は「1」～「30」）\n",
        "\n",
        "# NumPy多次元配列からテンソルに変換し、データ型は`float`に変換する\n",
        "t_X_train = torch.from_numpy(X_train).float()\n",
        "t_y_train = torch.from_numpy(y_train).float()\n",
        "t_X_valid = torch.from_numpy(X_valid).float()\n",
        "t_y_valid = torch.from_numpy(y_valid).float()\n",
        "\n",
        "# 「データ（X）」と「教師ラベル（y）」を、1つの「データセット（dataset）」にまとめる\n",
        "dataset_train = TensorDataset(t_X_train, t_y_train)  # 訓練用\n",
        "dataset_valid = TensorDataset(t_X_valid, t_y_valid)  # 精度検証用\n",
        "\n",
        "# ミニバッチを扱うための「データローダー（loader）」（訓練用と精度検証用）を作成\n",
        "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbevi-yv_iRg"
      },
      "source": [
        "このコードのポイント：\n",
        "- バッチサイズは学習時に扱うデータ単位であるが、`DataLoader`が「ミニバッチ」に関係するため、この段階で定義しておく必要がある\n",
        "- `DataLoader`クラスのコンストラクター引数`shuffle`で、データをシャッフルするか（**True**）しないか（**False**）を指定できる。今回はシャッフルしている"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFwgzpvAOsQo"
      },
      "source": [
        "## ■（6）ディープニューラルネットのモデル定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w3l4TB9CgvY"
      },
      "source": [
        "### リスト6-1　「1」か「-1」に分類するための「出力の離散化」\n",
        "- 今回のニューラルネットワークでは出力された確率値を、「**1**」か「**-1**」の2クラス分類値に離散化する\n",
        "  - 具体的には、0.0未満／0.0以上を-1.0／1.0にスケール変換する\n",
        "- そのための独自関数`discretize`を定義する\n",
        "- さらに、モデル内で扱いやすいように`torch.nn.Module`化も行っておく（※本稿では使用しない）\n",
        "- `discretize`関数は後述の「リスト7-3　1回分の「訓練（学習）」と「評価」の処理」で使用する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRN9Ox7UCtEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366f1395-fb5c-420b-cbdc-b25e3e6a1f5f"
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 離散化を行う単なる関数\n",
        "def discretize(proba):\n",
        "    '''\n",
        "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
        "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
        "  \n",
        "    Examples:\n",
        "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
        "        >>> binary = discretize(proba)\n",
        "    '''\n",
        "    threshold = torch.Tensor([0.0]) # -1か1かを分ける閾値を作成\n",
        "    discretized = (proba >= threshold).float() # 閾値未満で0、以上で1に変換\n",
        "    return discretized * 2 - 1.0 # 2倍して-1.0することで、0／1を-1.0／1.0にスケール変換\n",
        "\n",
        "# discretize関数をモデルで簡単に使用できるようにするため、\n",
        "# PyTorchの「torch.nn.Module」を継承したクラスラッパーも作成した\n",
        "class Discretize(nn.Module):\n",
        "    '''\n",
        "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
        "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
        "  \n",
        "    Examples:\n",
        "        >>> d = Discretize()\n",
        "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
        "        >>> binary = d(proba)\n",
        "    '''        \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # forward()メソッドは、基本クラス「torch.nn.Module」の__call__メソッドからも呼び出されるため、\n",
        "    # Discretizeオブジェクトを関数のように使える（例えば上記の「d(proba)」）\n",
        "    def forward(self, proba):\n",
        "        return discretize(proba) # 上記の関数を呼び出すだけ\n",
        "\n",
        "# 関数の利用をテスト\n",
        "proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)  # 確率値の例\n",
        "binary = discretize(proba)  # 2クラス分類（binary classification）値に離散化\n",
        "binary  # tensor([-1.,  1.,  1.]) …… などと表示される"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.,  1.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFvuSutuCgfq"
      },
      "source": [
        "このコードのポイント：\n",
        "- PyTrochのニューラルネットワークの基本を解説する内容ではないので、ざっと流して読めば十分（コメントを参考にしてほしい）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VT16L_p1o1R"
      },
      "source": [
        "### リスト6-2　ディープニューラルネットワークのモデル設計\n",
        "- 入力の数（`INPUT_FEATURES`）は、$X_1$と$X_2$で**2つ**\n",
        "- 隠れ層のレイヤー数は、**2つ**\n",
        "  - 隠れ層にある1つ目のニューロンの数（`LAYER1_NEURONS`）は、**3つ**\n",
        "  - 隠れ層にある2つ目のニューロンの数（`LAYER2_NEURONS`）は、**3つ**\n",
        "- 出力層にあるニューロンの数（`OUTPUT_RESULTS`）は、**1つ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28aZTzo_lFGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b153275-a57d-4ebe-ca6d-2258be8f4b14"
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 定数（モデル定義時に必要となるもの）\n",
        "INPUT_FEATURES = 2      # 入力（特徴）の数： 2\n",
        "LAYER1_NEURONS = 3      # ニューロンの数： 3\n",
        "LAYER2_NEURONS = 3      # ニューロンの数： 3\n",
        "OUTPUT_RESULTS = 1      # 出力結果の数： 1\n",
        "\n",
        "# 変数（モデル定義時に必要となるもの）\n",
        "activation1 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
        "activation2 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
        "acti_out = torch.nn.Tanh()     # 活性化関数（出力層用）： tanh関数（固定）\n",
        "\n",
        "# torch.nn.Moduleによるモデルの定義\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    # レイヤー（層）を定義\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # 隠れ層：1つ目のレイヤー（layer）\n",
        "        self.layer1 = nn.Linear(\n",
        "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
        "            LAYER1_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
        "\n",
        "        # 隠れ層：2つ目のレイヤー（layer）\n",
        "        self.layer2 = nn.Linear(\n",
        "            LAYER1_NEURONS,                # 入力ユニット数\n",
        "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
        "\n",
        "        # 出力層\n",
        "        self.layer_out = nn.Linear(\n",
        "            LAYER2_NEURONS,                # 入力ユニット数\n",
        "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
        "\n",
        "    # フォワードパスを定義\n",
        "    def forward(self, x):\n",
        "        # 「出力＝活性化関数（第n層（入力））」の形式で記述\n",
        "        x = activation1(self.layer1(x))  # 活性化関数は変数として定義\n",
        "        x = activation2(self.layer2(x))  # 同上\n",
        "        x = acti_out(self.layer_out(x))  # ※活性化関数は「tanh」固定\n",
        "        return x\n",
        "\n",
        "# モデル（NeuralNetworkクラス）のインスタンス化\n",
        "model = NeuralNetwork()\n",
        "model   # モデルの内容を出力"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (layer1): Linear(in_features=2, out_features=3, bias=True)\n",
              "  (layer2): Linear(in_features=3, out_features=3, bias=True)\n",
              "  (layer_out): Linear(in_features=3, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8smtM4J4F7x-"
      },
      "source": [
        "このコードのポイント：\n",
        "- 基本的な内容は、前掲の「リスト1-1　ニューロンのモデル設計」と同じ\n",
        "- 層が1つから、「隠れ層：2＋出力層：1」の3つに拡張されている\n",
        "- 例えば「LAYER1_NEURONS」に着目すると、1つ目のレイヤーにおける「出力ユニット数」であり、2つ目のレイヤーの「入力ユニット数」でもあるので、全く同じものが指定されている\n",
        "- フォワードプロパゲーション時のデータが変換されていく流れは、`forward`メソッド内に分かりやすく定義されている"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaaAwr4jDNG"
      },
      "source": [
        "## ■（7）学習／最適化（オプティマイザー）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95b3Pe4uOhgx"
      },
      "source": [
        "### リスト7-1　オプティマイザー（最適化用オブジェクト）の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMaORFyxw31H"
      },
      "source": [
        "import torch.optim as optim   # 「最適化」モジュールの別名定義\n",
        "\n",
        "# 定数（学習方法設計時に必要となるもの）\n",
        "LEARNING_RATE = 0.03   # 学習率： 0.03\n",
        "REGULARIZATION = 0.03  # 正則化率： 0.03\n",
        "\n",
        "# オプティマイザーを作成（パラメーターと学習率も指定）\n",
        "optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n",
        "    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n",
        "    lr=LEARNING_RATE,            # 更新時の学習率\n",
        "    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpjXYAQJYcHG"
      },
      "source": [
        "このコードのポイント：\n",
        "- この例では「SGD」（Stochastic Gradient Descent： 確率的勾配降下法）を選択\n",
        "- パラメーター（重みやバイアス）と、学習率、正則化率を引数に指定\n",
        "- 正則化（regularization）は「L2」に相当する。あまり使わない「L1」は基本機能に含まれていない\n",
        "- `torch.optim.SGD`を含めて以下が使用可能\n",
        "  - Adadelta\n",
        "  - Adagrad\n",
        "  - Adam（有名）\n",
        "  - AdamW\n",
        "  - SparseAdam\n",
        "  - Adamax\n",
        "  - ASGD\n",
        "  - LBFGS\n",
        "  - RMSprop\n",
        "  - Rprop\n",
        "  - SGD（確率的勾配降下法）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K48EyWqY8QCw"
      },
      "source": [
        "### リスト7-2　損失関数の定義\n",
        "定義した損失関数は次のリスト7-3で利用する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8luTeeRNBNi"
      },
      "source": [
        "# 変数（学習方法設計時に必要となるもの）\n",
        "criterion = nn.MSELoss()  # 損失関数：平均二乗誤差"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Bj8n7PQmvf"
      },
      "source": [
        "このコードのポイント：\n",
        "- `criterion`は慣例の変数名。誤差からの損失を測る「基準（criterion）」を意味する\n",
        "- `nn.MSELoss`も含めて以外が使用可能\n",
        "  - L1Loss（MAE：Mean Absolute Error、平均絶対誤差）\n",
        "  - MSELoss（MSE：Mean Squared Error、平均二乗誤差）\n",
        "  - CrossEntropyLoss（交差エントロピー誤差： クラス分類）\n",
        "  - CTCLoss\n",
        "  - NLLLoss\n",
        "  - PoissonNLLLoss\n",
        "  - KLDivLoss\n",
        "  - BCELoss\n",
        "  - BCEWithLogitsLoss\n",
        "  - MarginRankingLoss\n",
        "  - HingeEmbeddingLoss\n",
        "  - MultiLabelMarginLoss\n",
        "  - SmoothL1Loss\n",
        "  - SoftMarginLoss\n",
        "  - MultiLabelSoftMarginLoss\n",
        "  - CosineEmbeddingLoss\n",
        "  - MultiMarginLoss\n",
        "  - TripletMarginLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aehaIlYEhSWe"
      },
      "source": [
        "### リスト7-3　1回分の「訓練（学習）」と「評価」の処理\n",
        "`train_step`関数に訓練の内容を、`valid_step`関数に評価の内容を記述する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC8RzeLxapFK"
      },
      "source": [
        "def train_step(train_X, train_y):\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "\n",
        "    # フォワードプロパゲーションで出力結果を取得\n",
        "    #train_X                # 入力データ\n",
        "    pred_y = model(train_X) # 出力結果\n",
        "    #train_y                # 正解ラベル\n",
        "\n",
        "    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n",
        "    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n",
        "    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n",
        "\n",
        "    # 勾配を使ってパラメーター（重みとバイアス）を更新\n",
        "    optimizer.step()  # 指定されたデータ分の最適化を実施\n",
        "\n",
        "    # 正解率を算出\n",
        "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
        "        discr_y = discretize(pred_y)         # 確率値から「-1」／「1」に変換\n",
        "        acc = (discr_y == train_y).sum()     # 正解数を計算する\n",
        "\n",
        "    # 損失と正解数をタプルで返す\n",
        "    return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
        "\n",
        "def valid_step(valid_X, valid_y):\n",
        "    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n",
        "    model.eval()\n",
        "    \n",
        "    # フォワードプロパゲーションで出力結果を取得\n",
        "    #valid_X                # 入力データ\n",
        "    pred_y = model(valid_X) # 出力結果\n",
        "    #valid_y                # 正解ラベル\n",
        "\n",
        "    # 出力結果と正解ラベルから損失を計算\n",
        "    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "    # ※評価時は勾配を計算しない\n",
        "\n",
        "    # 正解率を算出\n",
        "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
        "        discr_y = discretize(pred_y)     # 確率値から「-1」／「1」に変換\n",
        "        acc = (discr_y == valid_y).sum() # 正解数を合計する\n",
        "\n",
        "    # 損失と正解数をタプルで返す\n",
        "    return (loss.item(), acc.item())  # ※item()=Pythonの数値"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIkBn4GQiKaM"
      },
      "source": [
        "このコードのポイント：\n",
        "- `model.eval()`メソッドを呼び出すと、評価（推論）モードとなり、（今回は使っていないが）BatchNormやDropoutなどの挙動が評価用になる。通常は、訓練モード（`model.train()`メソッド）になっている\n",
        "- `train_step`関数の処理内容： 「訓練モードの設定」「フォワードプロパゲーションで出力結果の取得」「出力結果と正解ラベルから損失および勾配の計算」「勾配を使ってパラメーター（重みとバイアス）の更新」「正解率の算出」\n",
        "- `valid_step`関数の処理内容： 「評価モードの設定」「フォワードプロパゲーションで出力結果の取得」「出力結果と正解ラベルから損失の計算」「正解率の算出」\n",
        "- フォワードプロパゲーションは、前掲の「リスト2-1　フォワードプロパゲーションの実行と結果確認」で説明済み\n",
        "- `with torch.no_grad():`の配下のテンソル計算のコードには自動微分用の勾配が生成されなくなり、メモリ使用量軽減やスピードアップなどの効果がある\n",
        "- `discretize`関数は前掲の『リスト6-1　「1」か「-1」に分類するための「出力の離散化」』で説明した"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-uPfRo22UT"
      },
      "source": [
        "### リスト7-4　「訓練」と「評価」をバッチサイズ単位でエポック回繰り返す\n",
        "`train_step`関数で訓練を、`valid_step`関数で評価を実行する。早期終了は基本機能ではないので今回は説明しない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG-yEd_qdETL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08e03df-588c-4111-80ff-8eaefc1eefe8"
      },
      "source": [
        "# パラメーター（重みやバイアス）の初期化を行う関数の定義\n",
        "def init_parameters(layer):\n",
        "    if type(layer) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
        "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
        "\n",
        "# 学習の前にパラメーター（重みやバイアス）を初期化する\n",
        "model.apply(init_parameters)\n",
        "\n",
        "# 定数（学習／評価時に必要となるもの）\n",
        "EPOCHS = 100             # エポック数： 100\n",
        "\n",
        "# 変数（学習／評価時に必要となるもの）\n",
        "avg_loss = 0.0           # 「訓練」用の平均「損失値」\n",
        "avg_acc = 0.0            # 「訓練」用の平均「正解率」\n",
        "avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n",
        "avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n",
        "\n",
        "# 損失の履歴を保存するための変数\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # forループ内で使う変数と、エポックごとの値リセット\n",
        "    total_loss = 0.0     # 「訓練」時における累計「損失値」\n",
        "    total_acc = 0.0      # 「訓練」時における累計「正解数」\n",
        "    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n",
        "    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n",
        "    total_train = 0      # 「訓練」時における累計「データ数」\n",
        "    total_valid = 0      # 「評価」時における累計「データ数」\n",
        "\n",
        "    for train_X, train_y in loader_train:\n",
        "        # 【重要】1ミニバッチ分の「訓練」を実行\n",
        "        loss, acc = train_step(train_X, train_y)\n",
        "\n",
        "        # 取得した損失値と正解率を累計値側に足していく\n",
        "        total_loss += loss          # 訓練用の累計損失値\n",
        "        total_acc += acc            # 訓練用の累計正解数\n",
        "        total_train += len(train_y) # 訓練データの累計数\n",
        "            \n",
        "    for valid_X, valid_y in loader_valid:\n",
        "        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n",
        "        val_loss, val_acc = valid_step(valid_X, valid_y)\n",
        "\n",
        "        # 取得した損失値と正解率を累計値側に足していく\n",
        "        total_val_loss += val_loss  # 評価用の累計損失値\n",
        "        total_val_acc += val_acc    # 評価用の累計正解数\n",
        "        total_valid += len(valid_y) # 訓練データの累計数\n",
        "\n",
        "    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n",
        "    n = epoch + 1                             # 処理済みのエポック数\n",
        "    avg_loss = total_loss / n                 # 訓練用の平均損失値\n",
        "    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n",
        "    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n",
        "    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n",
        "\n",
        "    # グラフ描画のために損失の履歴を保存する\n",
        "    train_history.append(avg_loss)\n",
        "    valid_history.append(avg_val_loss)\n",
        "\n",
        "    # 損失や正解率などの情報を表示\n",
        "    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
        "          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n",
        "          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')\n",
        "\n",
        "print('Finished Training')\n",
        "print(model.state_dict())  # 学習後のパラメーターの情報を表示"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch   1/100] loss: 23.34139, acc: 0.55600 val_loss: 21.33415, val_acc: 0.51600\n",
            "[Epoch   2/100] loss: 8.22904, acc: 0.62400 val_loss: 7.45542, val_acc: 0.61600\n",
            "[Epoch   3/100] loss: 4.45735, acc: 0.75200 val_loss: 4.48767, val_acc: 0.62000\n",
            "[Epoch   4/100] loss: 3.05675, acc: 0.75600 val_loss: 3.03754, val_acc: 0.82000\n",
            "[Epoch   5/100] loss: 2.31982, acc: 0.80400 val_loss: 2.26824, val_acc: 0.84000\n",
            "[Epoch   6/100] loss: 1.79722, acc: 0.80800 val_loss: 1.88286, val_acc: 0.80800\n",
            "[Epoch   7/100] loss: 1.46185, acc: 0.80400 val_loss: 1.47902, val_acc: 0.83200\n",
            "[Epoch   8/100] loss: 1.24441, acc: 0.82400 val_loss: 1.28513, val_acc: 0.79600\n",
            "[Epoch   9/100] loss: 1.08174, acc: 0.83200 val_loss: 1.09602, val_acc: 0.80400\n",
            "[Epoch  10/100] loss: 0.93439, acc: 0.81600 val_loss: 0.99172, val_acc: 0.78800\n",
            "[Epoch  11/100] loss: 0.83930, acc: 0.80800 val_loss: 0.87471, val_acc: 0.80400\n",
            "[Epoch  12/100] loss: 0.76330, acc: 0.83200 val_loss: 0.74919, val_acc: 0.83200\n",
            "[Epoch  13/100] loss: 0.68704, acc: 0.83600 val_loss: 0.71381, val_acc: 0.80800\n",
            "[Epoch  14/100] loss: 0.63662, acc: 0.84000 val_loss: 0.62469, val_acc: 0.83200\n",
            "[Epoch  15/100] loss: 0.58445, acc: 0.84400 val_loss: 0.60019, val_acc: 0.81200\n",
            "[Epoch  16/100] loss: 0.54801, acc: 0.82400 val_loss: 0.52979, val_acc: 0.83200\n",
            "[Epoch  17/100] loss: 0.49548, acc: 0.85600 val_loss: 0.48871, val_acc: 0.84000\n",
            "[Epoch  18/100] loss: 0.45439, acc: 0.84800 val_loss: 0.45784, val_acc: 0.83200\n",
            "[Epoch  19/100] loss: 0.42289, acc: 0.85600 val_loss: 0.41723, val_acc: 0.84800\n",
            "[Epoch  20/100] loss: 0.40032, acc: 0.83600 val_loss: 0.39075, val_acc: 0.85200\n",
            "[Epoch  21/100] loss: 0.37330, acc: 0.85600 val_loss: 0.36712, val_acc: 0.84000\n",
            "[Epoch  22/100] loss: 0.35157, acc: 0.84000 val_loss: 0.34745, val_acc: 0.84400\n",
            "[Epoch  23/100] loss: 0.32433, acc: 0.86400 val_loss: 0.30599, val_acc: 0.85600\n",
            "[Epoch  24/100] loss: 0.28841, acc: 0.87600 val_loss: 0.28358, val_acc: 0.86000\n",
            "[Epoch  25/100] loss: 0.26422, acc: 0.87200 val_loss: 0.26110, val_acc: 0.87600\n",
            "[Epoch  26/100] loss: 0.23484, acc: 0.89600 val_loss: 0.22838, val_acc: 0.91600\n",
            "[Epoch  27/100] loss: 0.21056, acc: 0.92400 val_loss: 0.23808, val_acc: 0.90800\n",
            "[Epoch  28/100] loss: 0.18356, acc: 0.96000 val_loss: 0.17497, val_acc: 0.97200\n",
            "[Epoch  29/100] loss: 0.15848, acc: 0.99200 val_loss: 0.15377, val_acc: 0.97600\n",
            "[Epoch  30/100] loss: 0.14214, acc: 0.99200 val_loss: 0.13740, val_acc: 0.99600\n",
            "[Epoch  31/100] loss: 0.12530, acc: 0.99600 val_loss: 0.12658, val_acc: 1.00000\n",
            "[Epoch  32/100] loss: 0.11479, acc: 0.99600 val_loss: 0.11127, val_acc: 1.00000\n",
            "[Epoch  33/100] loss: 0.10132, acc: 0.99200 val_loss: 0.10347, val_acc: 0.98000\n",
            "[Epoch  34/100] loss: 0.09301, acc: 1.00000 val_loss: 0.09269, val_acc: 0.98400\n",
            "[Epoch  35/100] loss: 0.08557, acc: 0.99200 val_loss: 0.10042, val_acc: 0.98800\n",
            "[Epoch  36/100] loss: 0.08018, acc: 1.00000 val_loss: 0.08092, val_acc: 0.99200\n",
            "[Epoch  37/100] loss: 0.07566, acc: 0.99600 val_loss: 0.07566, val_acc: 0.99600\n",
            "[Epoch  38/100] loss: 0.06987, acc: 0.99600 val_loss: 0.07324, val_acc: 0.99600\n",
            "[Epoch  39/100] loss: 0.06564, acc: 0.99600 val_loss: 0.07011, val_acc: 1.00000\n",
            "[Epoch  40/100] loss: 0.06222, acc: 0.99600 val_loss: 0.06438, val_acc: 1.00000\n",
            "[Epoch  41/100] loss: 0.05972, acc: 1.00000 val_loss: 0.06185, val_acc: 1.00000\n",
            "[Epoch  42/100] loss: 0.05533, acc: 0.99600 val_loss: 0.06396, val_acc: 1.00000\n",
            "[Epoch  43/100] loss: 0.05425, acc: 0.99600 val_loss: 0.05648, val_acc: 1.00000\n",
            "[Epoch  44/100] loss: 0.05140, acc: 0.99600 val_loss: 0.05212, val_acc: 1.00000\n",
            "[Epoch  45/100] loss: 0.04870, acc: 0.99600 val_loss: 0.05120, val_acc: 1.00000\n",
            "[Epoch  46/100] loss: 0.04660, acc: 0.99600 val_loss: 0.04788, val_acc: 1.00000\n",
            "[Epoch  47/100] loss: 0.04387, acc: 1.00000 val_loss: 0.04592, val_acc: 1.00000\n",
            "[Epoch  48/100] loss: 0.04192, acc: 0.99600 val_loss: 0.04439, val_acc: 1.00000\n",
            "[Epoch  49/100] loss: 0.04055, acc: 0.99600 val_loss: 0.04525, val_acc: 1.00000\n",
            "[Epoch  50/100] loss: 0.03935, acc: 1.00000 val_loss: 0.04055, val_acc: 1.00000\n",
            "[Epoch  51/100] loss: 0.03822, acc: 1.00000 val_loss: 0.03917, val_acc: 1.00000\n",
            "[Epoch  52/100] loss: 0.03717, acc: 1.00000 val_loss: 0.04012, val_acc: 1.00000\n",
            "[Epoch  53/100] loss: 0.03672, acc: 0.99600 val_loss: 0.03816, val_acc: 1.00000\n",
            "[Epoch  54/100] loss: 0.03343, acc: 1.00000 val_loss: 0.03716, val_acc: 0.99600\n",
            "[Epoch  55/100] loss: 0.03384, acc: 0.99600 val_loss: 0.03461, val_acc: 1.00000\n",
            "[Epoch  56/100] loss: 0.03301, acc: 1.00000 val_loss: 0.03550, val_acc: 1.00000\n",
            "[Epoch  57/100] loss: 0.03149, acc: 1.00000 val_loss: 0.03310, val_acc: 1.00000\n",
            "[Epoch  58/100] loss: 0.03133, acc: 1.00000 val_loss: 0.03239, val_acc: 1.00000\n",
            "[Epoch  59/100] loss: 0.03070, acc: 0.99600 val_loss: 0.03649, val_acc: 1.00000\n",
            "[Epoch  60/100] loss: 0.02971, acc: 1.00000 val_loss: 0.03267, val_acc: 1.00000\n",
            "[Epoch  61/100] loss: 0.02820, acc: 0.99600 val_loss: 0.03131, val_acc: 1.00000\n",
            "[Epoch  62/100] loss: 0.02789, acc: 1.00000 val_loss: 0.02972, val_acc: 1.00000\n",
            "[Epoch  63/100] loss: 0.02797, acc: 1.00000 val_loss: 0.02926, val_acc: 1.00000\n",
            "[Epoch  64/100] loss: 0.02694, acc: 1.00000 val_loss: 0.02846, val_acc: 1.00000\n",
            "[Epoch  65/100] loss: 0.02648, acc: 1.00000 val_loss: 0.02737, val_acc: 1.00000\n",
            "[Epoch  66/100] loss: 0.02622, acc: 1.00000 val_loss: 0.02850, val_acc: 1.00000\n",
            "[Epoch  67/100] loss: 0.02465, acc: 1.00000 val_loss: 0.02661, val_acc: 1.00000\n",
            "[Epoch  68/100] loss: 0.02390, acc: 1.00000 val_loss: 0.02661, val_acc: 1.00000\n",
            "[Epoch  69/100] loss: 0.02443, acc: 1.00000 val_loss: 0.02650, val_acc: 1.00000\n",
            "[Epoch  70/100] loss: 0.02373, acc: 0.99600 val_loss: 0.02464, val_acc: 1.00000\n",
            "[Epoch  71/100] loss: 0.02372, acc: 1.00000 val_loss: 0.02463, val_acc: 1.00000\n",
            "[Epoch  72/100] loss: 0.02379, acc: 0.99600 val_loss: 0.02431, val_acc: 1.00000\n",
            "[Epoch  73/100] loss: 0.02263, acc: 1.00000 val_loss: 0.02449, val_acc: 1.00000\n",
            "[Epoch  74/100] loss: 0.02225, acc: 0.99600 val_loss: 0.02327, val_acc: 1.00000\n",
            "[Epoch  75/100] loss: 0.02167, acc: 1.00000 val_loss: 0.02335, val_acc: 1.00000\n",
            "[Epoch  76/100] loss: 0.02157, acc: 1.00000 val_loss: 0.02232, val_acc: 1.00000\n",
            "[Epoch  77/100] loss: 0.02080, acc: 1.00000 val_loss: 0.02235, val_acc: 1.00000\n",
            "[Epoch  78/100] loss: 0.02049, acc: 1.00000 val_loss: 0.02127, val_acc: 1.00000\n",
            "[Epoch  79/100] loss: 0.02016, acc: 1.00000 val_loss: 0.02296, val_acc: 1.00000\n",
            "[Epoch  80/100] loss: 0.01929, acc: 1.00000 val_loss: 0.02109, val_acc: 1.00000\n",
            "[Epoch  81/100] loss: 0.01947, acc: 0.99600 val_loss: 0.02209, val_acc: 1.00000\n",
            "[Epoch  82/100] loss: 0.01916, acc: 1.00000 val_loss: 0.02007, val_acc: 1.00000\n",
            "[Epoch  83/100] loss: 0.01897, acc: 1.00000 val_loss: 0.01956, val_acc: 1.00000\n",
            "[Epoch  84/100] loss: 0.01886, acc: 1.00000 val_loss: 0.01941, val_acc: 1.00000\n",
            "[Epoch  85/100] loss: 0.01804, acc: 1.00000 val_loss: 0.02340, val_acc: 1.00000\n",
            "[Epoch  86/100] loss: 0.01831, acc: 1.00000 val_loss: 0.01995, val_acc: 1.00000\n",
            "[Epoch  87/100] loss: 0.01817, acc: 1.00000 val_loss: 0.01914, val_acc: 1.00000\n",
            "[Epoch  88/100] loss: 0.01790, acc: 1.00000 val_loss: 0.01888, val_acc: 1.00000\n",
            "[Epoch  89/100] loss: 0.01775, acc: 1.00000 val_loss: 0.01864, val_acc: 1.00000\n",
            "[Epoch  90/100] loss: 0.01732, acc: 1.00000 val_loss: 0.01842, val_acc: 1.00000\n",
            "[Epoch  91/100] loss: 0.01690, acc: 1.00000 val_loss: 0.01753, val_acc: 1.00000\n",
            "[Epoch  92/100] loss: 0.01696, acc: 1.00000 val_loss: 0.01855, val_acc: 1.00000\n",
            "[Epoch  93/100] loss: 0.01653, acc: 1.00000 val_loss: 0.01804, val_acc: 1.00000\n",
            "[Epoch  94/100] loss: 0.01621, acc: 1.00000 val_loss: 0.01680, val_acc: 1.00000\n",
            "[Epoch  95/100] loss: 0.01646, acc: 1.00000 val_loss: 0.01667, val_acc: 1.00000\n",
            "[Epoch  96/100] loss: 0.01523, acc: 1.00000 val_loss: 0.01776, val_acc: 0.99600\n",
            "[Epoch  97/100] loss: 0.01649, acc: 0.99600 val_loss: 0.01655, val_acc: 1.00000\n",
            "[Epoch  98/100] loss: 0.01555, acc: 1.00000 val_loss: 0.01584, val_acc: 1.00000\n",
            "[Epoch  99/100] loss: 0.01549, acc: 1.00000 val_loss: 0.01575, val_acc: 1.00000\n",
            "[Epoch 100/100] loss: 0.01534, acc: 1.00000 val_loss: 0.01601, val_acc: 1.00000\n",
            "Finished Training\n",
            "OrderedDict([('layer1.weight', tensor([[-0.4816, -0.2574],\n",
            "        [ 0.0520, -0.5455],\n",
            "        [ 0.5311, -0.2319]])), ('layer1.bias', tensor([-1.0360,  0.9736, -1.0185])), ('layer2.weight', tensor([[-0.1287, -1.2505,  0.6586],\n",
            "        [ 0.1831, -0.3140,  0.1388],\n",
            "        [ 1.2934, -0.0693,  0.9491]])), ('layer2.bias', tensor([0.2445, 0.2515, 0.6265])), ('layer_out.weight', tensor([[-1.5046, -0.4190, -1.7195]])), ('layer_out.bias', tensor([-1.1955]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8wBgA6LYmk"
      },
      "source": [
        "このコードのポイント：\n",
        "- 重要なのは、「リスト7-3　1回分の「訓練（学習）」と「評価」の処理」で定義した`train_step`関数と`valid_step`関数の呼び出しだけである\n",
        "- 1つ目のforループでエポックを回している\n",
        "  - 2つ目のforループでバッチ単位分のデータを処理に渡している（ミニバッチ処理）\n",
        "- たくさんの変数があって長いが、どれも表示用の損失や正解率を計算するための処理であり、ニューラルネットワークの本質ではない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz1U1sOTaDiN"
      },
      "source": [
        "## ■（8）評価／精度検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl7Cau6jGqF6"
      },
      "source": [
        "### リスト8-1　損失値の推移グラフ描画"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWlji88SdXpS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3ffea176-66bc-4e5f-ead6-486c1ad05402"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習結果（損失）のグラフを描画\n",
        "epochs = len(train_history)\n",
        "plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n",
        "plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSsuIVHLTn4"
      },
      "source": [
        "このコードのポイント：\n",
        "- `train_history`と`valid_history`は、前述の「リスト7-4　「訓練」と「評価」をバッチサイズ単位でエポック回繰り返す」で記録しておいた損失の履歴である\n",
        "- プロットする方法はいたって普通なので、特に本稿では説明しない"
      ]
    }
  ]
}